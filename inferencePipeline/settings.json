{
    "model": {
        "name": "meta-llama/Llama-3.1-8B-Instruct",
        "cache_dir": "/app/models",
        "gguf_cache_dir": "./gguf_cache"
    },
    "server": {
        "host": "127.0.0.1",
        "port": 8081,
        "model_file": "model_llama_3_1_8b_q4_0.gguf",
        "context_size": 65536,
        "n_gpu_layers": -1,
        "parallel_requests": 8,
        "continuous_batching": true,
        "max_workers": 8
    },
    "local": {
        "device": "cuda",
        "torch_dtype": "bfloat16",
        "batch_size": 64,
        "use_torch_compile": false,
        "compile_mode": "reduce-overhead",
        "low_cpu_mem_usage": true
    },
    "inference": {
        "token_limits": {
            "algebra": 180,
            "history": 150,
            "geography": 100,
            "general": 120
        },
        "generation": {
            "temperature": 1.0,
            "do_sample": false,
            "num_beams": 1,
            "top_p": 1.0,
            "top_k": 1,
            "repetition_penalty": 1.0,
            "length_penalty": 1.0
        },
        "subject_detection": {
            "algebra_keywords": [
                "solve",
                "equation",
                "factor",
                "matrix",
                "simplify",
                "function",
                "graph",
                "calculate",
                "variable"
            ],
            "geography_keywords": [
                "country",
                "capital",
                "mountain",
                "river",
                "ocean",
                "located",
                "land",
                "region",
                "world"
            ],
            "history_keywords": [
                "war",
                "revolution",
                "when did",
                "who was",
                "industrial",
                "century",
                "ancient",
                "influence",
                "period"
            ],
            "min_keyword_matches": 1
        },
        "few_shot": {
            "examples_per_subject": 3,
            "use_few_shot": true
        }
    },
    "gguf_conversion": {
        "quantization": "q4_0",
        "output_filename": "model_llama_3_1_8b_q4_0.gguf",
        "f16_intermediate": true
    },
    "optimization": {
        "gc_frequency": 192,
        "max_answer_length": 5000,
        "request_timeout_seconds": 30
    }
}