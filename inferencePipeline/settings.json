{
    "model": {
        "name": "meta-llama/Llama-3.2-1B-Instruct",
        "cache_dir": "./app/model",
        "gguf_cache_dir": "./gguf_cache"
    },
    "server": {
        "host": "127.0.0.1",
        "port": 8081,
        "model_file": "model_q4_0_draft.gguf",
        "context_size": 2048,
        "n_gpu_layers": 0,
        "parallel_requests": 8,
        "continuous_batching": true,
        "max_workers": 8
    },
    "local": {
        "device": "cpu",
        "torch_dtype": "bfloat16",
        "batch_size": 64,
        "use_torch_compile": true,
        "compile_mode": "reduce-overhead",
        "low_cpu_mem_usage": true
    },
    "inference": {
        "token_limits": {
            "algebra": 180,
            "history": 150,
            "geography": 100,
            "general": 120
        },
        "generation": {
            "temperature": 0.0,
            "do_sample": false,
            "num_beams": 1,
            "top_p": 1.0,
            "top_k": 1
        }
    },
    "gguf_conversion": {
        "target_quantizations": {
            "main": "q8_0",
            "draft": "q4_0"
        },
        "f16_intermediate": true
    }
}