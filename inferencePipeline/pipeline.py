"""
Tech Arena 2025 - Phase 2
Efficient LLM Inference Pipeline with vLLM

OPTIMIZATIONS:
âœ… vLLM for fast inference with Qwen 4B
âœ… FP16 precision for T4 GPU (16GB)
âœ… Memory-optimized batch processing for T4 constraints
âœ… Batched processing by subject
âœ… Few-shot prompting for Chinese and Algebra
âœ… Answer extraction (returns only final answers, not reasoning)
âœ… Prefix caching for few-shot examples
âœ… CPU swap space for memory overflow handling
âœ… Speculative decoding with draft model for 2-3x speedup
"""

import os
import re
from typing import List, Dict
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
from pathlib import Path



# Configuration
# Model options: "Qwen/Qwen3-8B" (recommended), "Qwen/Qwen3-4B", or "meta-llama/Llama-3.1-8B-Instruct"
MODEL_NAME = os.environ.get("MODEL_NAME", "Qwen/Qwen3-4B")
CACHE_DIR = "/app/models"
CHINESE_KB_PATH = os.path.join(os.path.dirname(__file__), "chinese_kb.txt")
ALGEBRA_KB_PATH = os.path.join(os.path.dirname(__file__), "algebra_kb.txt")

# Speculative decoding configuration
# Use smaller draft model for faster token generation with verification
ENABLE_SPECULATIVE_DECODING = os.environ.get("ENABLE_SPECULATIVE_DECODING", "true").lower() == "true"
DRAFT_MODEL_NAME = os.environ.get("DRAFT_MODEL_NAME", "Qwen/Qwen3-1.7B")  # Fast draft model (1.7B for better accuracy)
SPECULATIVE_MAX_MODEL_LEN = 2048  # Shorter for draft model to save memory

# Model-specific settings
MODEL_CONFIGS = {
    "Qwen/Qwen3-4B": {
        "dtype": "half",  # FP16
        "quantization": "awq",
        "gpu_memory_utilization": 0.90,
        "use_prequantized": False,
    },
    "Qwen/Qwen3-8B": {
        "dtype": "half",  # FP16
        "quantization": "awq",  # 4-bit quantization required for T4
        "gpu_memory_utilization": 0.88,
    },
    "meta-llama/Llama-3.2-3B-Instruct": {
        "dtype": "half",  # FP16
        "quantization": None,  # âœ… Run in native FP16 (Fits easily on T4!)
        "gpu_memory_utilization": 0.90,
        "use_prequantized": False,
    },
    "meta-llama/Llama-3.1-8B-Instruct": {
        "dtype": "half",
        "quantization": "awq",  # âœ… Enabled - FP16 OOMs on T4!
        "gpu_memory_utilization": 0.85,
        "use_prequantized": False,
    }
}


def find_model_path(model_name: str, cache_dir: str) -> str:
    """Find the actual snapshot path in HuggingFace cache"""
    cache_path = Path(cache_dir)
    
    # Convert model name to HF cache format: meta-llama/Llama-3.2-3B-Instruct -> models--meta-llama--Llama-3.2-3B-Instruct
    hf_cache_name = "models--" + model_name.replace("/", "--")
    model_cache = cache_path / hf_cache_name
    
    if not model_cache.exists():
        raise FileNotFoundError(f"Model cache not found at {model_cache}")
    
    # Find the snapshot directory (usually only one)
    snapshots_dir = model_cache / "snapshots"
    if not snapshots_dir.exists():
        raise FileNotFoundError(f"Snapshots directory not found at {snapshots_dir}")
    
    # Get the latest (or only) snapshot
    snapshots = list(snapshots_dir.iterdir())
    if not snapshots:
        raise FileNotFoundError(f"No snapshots found in {snapshots_dir}")
    
    # Use the most recent snapshot
    snapshot = sorted(snapshots, key=lambda p: p.stat().st_mtime, reverse=True)[0]
    return str(snapshot)


RAW_MODEL_PATH = find_model_path(MODEL_NAME, CACHE_DIR)


def quantize_model_awq(model_path: str, cache_dir: str, model_name: str) -> str:
    """
    Quantize model to AWQ 4-bit format (one-time operation during untimed setup)
    
    Returns path to quantized model (either newly created or existing cached version)
    """
    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer
    
    # Define quantized model path dynamically
    safe_name = model_name.split("/")[-1].lower().replace("-", "_")
    quant_path = os.path.join(cache_dir, f"{safe_name}_awq")
    
    # Check if already quantized
    if os.path.exists(quant_path) and os.path.exists(os.path.join(quant_path, "config.json")):
        print(f"âœ… AWQ quantized model found at {quant_path}")
        return quant_path
    
    print(f"ğŸ”§ AWQ model not found. Starting quantization (one-time setup)...")
    print(f"   Source: {model_path}")
    print(f"   Target: {quant_path}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # Load model for quantization with MEMORY OPTIMIZATIONS
    # T4 has 15.3GB VRAM, 8B FP16 model is ~16GB â†’ need CPU offloading
    print("âš™ï¸  Loading model with memory optimizations (CPU offloading enabled)...")
    model = AutoAWQForCausalLM.from_pretrained(
        model_path,
        low_cpu_mem_usage=True,  # âœ… Reduce CPU RAM usage
        max_memory={0: "13GB", "cpu": "30GB"},  # âœ… Limit GPU to 13GB, use CPU for overflow
        offload_folder="offload_tmp",  # âœ… Temporary offload directory
        safetensors=True
    )
    
    # AWQ quantization config (4-bit, group size 128)
    quant_config = {
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"
    }
    
    # Domain-specific calibration data (REDUCED to 128 samples - AWQ only needs 128-256)
    # Tailored to Tech Arena tasks: Chinese, Algebra, and General Knowledge
    calibration_data = [
        # Chinese (General & Cultural)
        "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚",
        "é•¿åŸæ˜¯ä¸–ç•Œè‘—åçš„å¤ä»£å»ºç­‘å¥‡è¿¹ã€‚",
        "è¯·è§£é‡Šä¸€ä¸‹é‡å­åŠ›å­¦çš„åŸºæœ¬åŸç†ã€‚",
        "å”æœæ˜¯ä¸­å›½å†å²ä¸Šæœ€ç¹è£çš„æœä»£ä¹‹ä¸€ã€‚",
        "å¦‚ä½•åˆ¶ä½œä¸€é“æ­£å®—çš„å®«ä¿é¸¡ä¸ï¼Ÿ",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¦‚ä½•ï¼Ÿ",
        "å¤ªé˜³ç³»ä¸­æœ‰å…«å¤§è¡Œæ˜Ÿï¼Œåœ°çƒæ˜¯ç¬¬ä¸‰é¢—ã€‚",
        "æç™½æ˜¯å”æœè‘—åçš„æµªæ¼«ä¸»ä¹‰è¯—äººã€‚",
        
        # Algebra & Math (Equations, Functions, Word Problems)
        "Solve for x: 3x + 7 = 22",
        "Calculate the derivative of f(x) = x^3 - 4x + 1",
        "If a train travels at 60 mph for 2.5 hours, how far does it go?",
        "Simplify the expression: (a^2 - b^2) / (a - b)",
        "Find the roots of the quadratic equation: x^2 - 5x + 6 = 0",
        "What is the area of a circle with radius 5?",
        "Solve the system of equations: 2x + y = 10, x - y = 2",
        "Calculate the integral of sin(x) from 0 to pi.",
        "If log_10(x) = 2, what is x?",
        
        # General Knowledge (History, Geography, Science)
        "The Industrial Revolution began in Great Britain in the 18th century.",
        "Photosynthesis is the process by which plants convert light into energy.",
        "The Amazon Rainforest is often referred to as the 'lungs of the Earth'.",
        "Albert Einstein developed the theory of relativity.",
        "The Great Barrier Reef is located off the coast of Australia.",
        "Water boils at 100 degrees Celsius at sea level.",
        "The United Nations was established in 1945 to promote world peace.",
        "DNA replication is a fundamental process in cell division.",
        "The currency of Japan is the Yen.",
        "Marie Curie was the first woman to win a Nobel Prize.",
    ] * 5  # Repeat to get 130 samples (optimal for AWQ)
    
    print(f"ğŸ“Š Using {len(calibration_data)} calibration samples (optimized for AWQ)")
    
    # Quantize the model
    print("âš™ï¸  Quantizing model (this may take 5-10 minutes)...")
    model.quantize(
        tokenizer, 
        quant_config=quant_config, 
        calib_data=calibration_data
    )
    
    # Save quantized model
    print(f"ğŸ’¾ Saving quantized model to {quant_path}...")
    os.makedirs(quant_path, exist_ok=True)
    model.save_quantized(quant_path, safetensors=True, shard_size="4GB")
    tokenizer.save_pretrained(quant_path)
    
    print(f"âœ… AWQ quantization complete! Model saved to {quant_path}")
    
    # CLEANUP: Free GPU memory immediately
    del model
    del tokenizer
    import gc
    import torch
    import shutil
    gc.collect()
    torch.cuda.empty_cache()
    
    # Remove offload directory if it exists
    if os.path.exists("offload_tmp"):
        shutil.rmtree("offload_tmp")
        print("ğŸ§¹ Cleaned up offload directory")
    
    print("ğŸ§¹ Freed GPU memory after quantization")
    
    return quant_path



class InferencePipeline:
    """
    Runtime-quantized inference pipeline for T4 GPU

    Strategy:
    1. Quantize model during untimed setup (loadPipeline)
    2. Batch questions by subject with reasoning control
    3. Enable reasoning for Chinese and Algebra
    4. Maximum throughput with vLLM
    """

    def __init__(self):
        """
        Initialize pipeline with AWQ-quantized vLLM (4-bit)
        AWQ quantization happens during untimed setup for memory efficiency
        """

        # Load knowledge bases for CAG
        self.chinese_kb = self._load_chinese_knowledge_base()
        self.algebra_kb = self._load_algebra_knowledge_base()

        # Get model-specific configuration
        model_config = MODEL_CONFIGS.get(MODEL_NAME, MODEL_CONFIGS["Qwen/Qwen3-4B"])

        # Optimized configuration for T4 16GB GPU (speed + stability)
        model_display = MODEL_NAME.split("/")[-1]
        quant_info = f" ({model_config['quantization']})" if model_config['quantization'] else " (FP16)"
        print(f"ğŸš€ Loading {model_display} with vLLM{quant_info}...")

        # Determine if we need to quantize first (for 4B AWQ legacy path or if config demands it)
        # Note: The user's new code assumes 8B AWQ is pre-quantized or handled differently.
        # We will keep the existing AWQ logic for 4B if selected, but adapt for the new config structure.
        
        # Determine if we need to quantize first (for AWQ models)
        if model_config['quantization'] == 'awq':
             print(f"ğŸš€ Preparing AWQ quantized model for {MODEL_NAME}...")
             awq_model_path = quantize_model_awq(RAW_MODEL_PATH, CACHE_DIR, MODEL_NAME)
             model_path = awq_model_path
        else:
             model_path = RAW_MODEL_PATH

        print("Using model path: ", model_path)

        # Speculative decoding setup
        speculative_config = None
        if ENABLE_SPECULATIVE_DECODING:
            print(f"ğŸš€ Setting up speculative decoding with draft model: {DRAFT_MODEL_NAME}")
            # Find draft model path
            try:
                draft_model_path = find_model_path(DRAFT_MODEL_NAME, CACHE_DIR)
                speculative_config = {
                    "draft_model": draft_model_path,
                    "num_speculative_tokens": 5,  # Number of tokens to speculate ahead
                }
                print(f"âœ… Draft model loaded from: {draft_model_path}")
            except Exception as e:
                print(f"âš ï¸  Failed to load draft model, disabling speculative decoding: {e}")
                speculative_config = None

        # Build LLM arguments
        llm_args = {
            "model": model_path,
            "quantization": model_config['quantization'],
            "dtype": model_config['dtype'],
            "gpu_memory_utilization": model_config['gpu_memory_utilization'] - 0.05 if speculative_config else model_config['gpu_memory_utilization'],  # Reserve memory for draft model
            "max_model_len": 16384,              # Increased for Full Context CAG (Chinese KB is ~8k tokens)
            "enforce_eager": False,
            "max_num_seqs": 16,                  # Reduced slightly to save VRAM for large context
            "max_num_batched_tokens": 16384,     # Match model len
            "enable_prefix_caching": True,
            "trust_remote_code": True,
            "tensor_parallel_size": 1,
            "swap_space": 4,
            "disable_log_stats": True,
        }

        # Add speculative decoding if enabled
        if speculative_config:
            llm_args["speculative_config"] = speculative_config
            print("âœ… Speculative decoding enabled!")

        self.llm = LLM(**llm_args)

        self.tokenizer = self.llm.get_tokenizer()
        
        # Warmup the cache with Knowledge Bases
        self._warmup_cache()

        # Sampling parameters for Chinese (optimized)
        self.params_chinese = SamplingParams(
            temperature=0.18,  # Slightly lower for speed
            top_p=0.88,
            max_tokens=300,    # Slightly reduced
            stop=["<|im_end|>", "\n\né—®é¢˜", "ç­”æ¡ˆ:", "\n\n\n"],
            skip_special_tokens=True,
        )

        # Sampling parameters for Algebra (self-consistency for accuracy)
        self.params_algebra = SamplingParams(
            temperature=0.3,   # Higher for diverse solutions
            top_p=0.95,        # High for complex reasoning
            max_tokens=600,    # Optimized for algebra complexity
            #n=3,               # Generate 3 candidate solutions
            stop=["<|im_end|>", "\n\nProblem:", "\n\nExample", "\n\n\n"],
            skip_special_tokens=True,
        )

        # Sampling parameters for History/Geography/Finance
        self.params_general = SamplingParams(
            temperature=0.2,   # Slightly higher to avoid getting stuck
            top_p=0.92,
            max_tokens=448,    # Increased for complete answers (especially History)
            stop=["<|im_end|>", "\n\nQuestion:", "\n\nExample", "\n\n\n"],
            skip_special_tokens=True,
        )

        print("âœ… Pipeline ready for inference\n")

    def _strip_thinking(self, text: str) -> str:
        """Remove <think> tags and their content from the output"""
        import re
        # Remove <think>...</think> blocks
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        # Remove standalone <think> or </think> tags
        text = re.sub(r'</?think>', '', text)
        return text.strip()

    def _majority_vote(self, candidates: List[str]) -> str:
        """Pick most common answer from multiple candidates (self-consistency)"""
        from collections import Counter

        # Normalize candidates (strip whitespace, lowercase for comparison)
        normalized = [c.strip().lower() for c in candidates if c.strip()]

        if not normalized:
            return candidates[0] if candidates else ""

        # Count occurrences
        counter = Counter(normalized)
        most_common = counter.most_common(1)[0][0]

        # Return original (non-normalized) version
        for i, norm in enumerate(normalized):
            if norm == most_common:
                return candidates[i].strip()

        return candidates[0]


    def _create_chat_prompt(self, question: str, subject: str = "general") -> str:
        """Create prompt using Qwen chat template with few-shot examples"""

        if subject == "chinese":
            # Full Context CAG for Chinese questions
            # We inject the ENTIRE Knowledge Base.
            # Because of _warmup_cache, this prefix is already cached!
            prompt = f"""ä½ æ˜¯ä¸­å›½æ–‡åŒ–å’Œè¯­è¨€ä¸“å®¶ã€‚ä½¿ç”¨ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

å‚è€ƒèµ„æ–™ï¼š
{self.chinese_kb}

åŸºäºä»¥ä¸Šå‚è€ƒèµ„æ–™ï¼Œç›´æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜: {question}
ç­”æ¡ˆ:"""

        elif subject == "algebra":
            # Full Context CAG + Chain-of-Thought for algebra
            # Inject ENTIRE Algebra KB. Cached via warmup.
            prompt = f"""You are a math expert. Use the formulas and theorems below to solve the problem.

            REFERENCE FORMULAS:
            {self.algebra_kb}

            Using the above formulas, solve step-by-step and provide the final answer.

            Example 1:
            Problem: Solve for x: 2x + 5 = 13
            Solution: Subtract 5 from both sides: 2x = 8. Divide by 2: x = 4.
            Final Answer: x = 4

            Example 2:
            Problem: Simplify (x+3)(x-3)
            Solution: Using difference of squares formula (a+b)(a-b) = aÂ² - bÂ². Here a=x, b=3.
            Final Answer: xÂ² - 9

            Example 3:
            Problem: If f(x) = 3xÂ² - 2x + 1, find f(2)
            Solution: Substitute x=2 into the function: f(2) = 3(2)Â² - 2(2) + 1 = 3(4) - 4 + 1 = 12 - 4 + 1 = 9.
            Final Answer: 9

            Example 4:
            Problem: What is the derivative of xÂ² + 3x?
            Solution: Using power rule: d/dx(xÂ²) = 2x, d/dx(3x) = 3. Sum: 2x + 3.
            Final Answer: 2x + 3

            Example 5:
            Problem: Solve the system: x + y = 5, x - y = 1
            Solution: Add equations: (x+y) + (x-y) = 5+1, so 2x = 6, x = 3. Substitute into first equation: 3 + y = 5, y = 2.
            Final Answer: x = 3, y = 2

            Example 6:
            Problem: Find the area of a circle with radius 5
            Solution: Use formula A = Ï€rÂ². A = Ï€(5)Â² = 25Ï€ â‰ˆ 78.54.
            Final Answer: 25Ï€ or approximately 78.54

            Example 7:
            Problem: Factor xÂ² + 5x + 6
            Solution: Find two numbers that multiply to 6 and add to 5: 2 and 3. So (x+2)(x+3).
            Final Answer: (x+2)(x+3)

            Example 8:
            Problem: What is 15% of 80?
            Solution: 15% = 0.15. Multiply: 0.15 Ã— 80 = 12.
            Final Answer: 12

            Now solve this problem step-by-step:

            Problem: {question}
            Solution:"""



        else:
            # History/Geography/Finance - Direct answer format
            prompt = f"""Answer this question directly with just the factual answer.

Question: Who discovered America?
Answer: Christopher Columbus in 1492.

Question: What is the highest point in South America?
Answer: Mount Aconcagua in Argentina (6,961 meters).

Question: Which country is known as the 'Land of the Thunder Dragon'?
Answer: Bhutan.

Question: Who won World War 2?
Answer: The Allied Powers (United States, Soviet Union, United Kingdom, and France).

Question: {question}
Answer:"""

        messages = [{"role": "user", "content": prompt}]
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

    def _extract_final_answer(self, text: str, subject: str) -> str:
        """Extract only the final answer from reasoning output"""

        if subject == 'algebra':
            # Strip any reasoning/thinking prefixes
            # Remove lines starting with "Okay", "So", "Let me", etc.
            reasoning_prefixes = [
                r'^(?:Okay|So|Let me|First|Now|Here|The user|To solve|We need).*?\n',
                r'^(?:I need to|I will|Let\'s|Step \d+).*?\n',
            ]
            for prefix in reasoning_prefixes:
                text = re.sub(prefix, '', text, flags=re.IGNORECASE | re.MULTILINE)

            # Aggressive extraction - prioritize "Answer:" pattern
            answer_patterns = [
                r'Answer:\s*(.+?)(?:\n\n|\n(?=Problem)|$)',  # Most specific
                r'(?:Final [Aa]nswer|ANSWER):\s*(.+?)(?:\n|$)',
                r'(?:Therefore|Thus|So),?\s*(.+?)(?:\n|$)',
                r'=\s*([^=\n]+)$',  # Last equation
            ]

            for pattern in answer_patterns:
                match = re.search(pattern, text, re.MULTILINE | re.DOTALL)
                if match:
                    answer = match.group(1).strip()
                    # Clean up artifacts
                    answer = answer.rstrip('.').strip()
                    # Remove any trailing explanations
                    answer = answer.split('\n')[0]
                    return answer

            # Fallback: return last non-empty line that looks like an answer
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            for line in reversed(lines):
                # Skip lines that look like reasoning
                if not any(line.lower().startswith(word) for word in ['so', 'therefore', 'thus', 'let', 'we', 'the']):
                    return line

            if lines:
                return lines[-1]

        elif subject == 'chinese':
            # Chinese-specific extraction
            answer_patterns = [
                r'(?:ç­”æ¡ˆ|æœ€ç»ˆç­”æ¡ˆ)[:ï¼š]\s*(.+?)(?:\n\n|\n|$)',
                r'(?:Answer|answer):\s*(.+?)(?:\n\n|\n|$)',
                r'(?:ç»“è®º|å› æ­¤)[:ï¼š,ï¼Œ]\s*(.+?)(?:\n\n|\n|$)',
            ]

            for pattern in answer_patterns:
                match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                if match:
                    answer = match.group(1).strip()
                    answer = re.sub(r'\n+', ' ', answer)
                    return answer

            lines = [line.strip() for line in text.split('\n') if line.strip()]
            if lines:
                return lines[-1]

        # For other subjects or if extraction fails, return the full text
        return text

    def __call__(self, questions: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Main inference method with reasoning-based batching

        Reasoning enabled for: Chinese, Algebra
        No reasoning for: History, Geography
        """

        if not questions:
            return []

        results = [None] * len(questions)

        # =====================================================================
        # PHASE 1: BATCH SORTING BY SUBJECT
        # =====================================================================

        chinese_indices = []
        chinese_prompts = []

        algebra_indices = []
        algebra_prompts = []

        general_indices = []
        general_prompts = []

        for i, q in enumerate(questions):
            subject = q.get('subject', 'general').lower()
            prompt = self._create_chat_prompt(q['question'], subject=subject)

            if subject == 'chinese':
                chinese_indices.append(i)
                chinese_prompts.append(prompt)
            elif subject == 'algebra':
                algebra_indices.append(i)
                algebra_prompts.append(prompt)
            else:
                general_indices.append(i)
                general_prompts.append(prompt)

        # =====================================================================
        # PHASE 2: BATCH EXECUTION BY SUBJECT
        # =====================================================================

        # Batch A: Chinese questions
        if chinese_prompts:
            print(f"ğŸ‡¨ğŸ‡³ Processing {len(chinese_prompts)} Chinese questions...")

            chinese_outputs = self.llm.generate(chinese_prompts, self.params_chinese, use_tqdm=False)

            for idx, output in zip(chinese_indices, chinese_outputs):
                raw_answer = output.outputs[0].text.strip()
                raw_answer = self._strip_thinking(raw_answer)
                answer = self._extract_final_answer(raw_answer, 'chinese')

                if len(answer) > 5000:
                    answer = answer[:5000].rsplit('. ', 1)[0] + '.'

                results[idx] = {
                    "questionID": questions[idx]["questionID"],
                    "answer": answer
                }

        # Batch B: Algebra questions (self-consistency for accuracy)
        if algebra_prompts:
            print(f"ğŸ”¢ Processing {len(algebra_prompts)} Algebra questions (3x for accuracy)...")

            algebra_outputs = self.llm.generate(algebra_prompts, self.params_algebra, use_tqdm=False)

            for idx, output in zip(algebra_indices, algebra_outputs):
                # Get all 3 candidate solutions (n=3 in params)
                candidates = []
                for candidate_output in output.outputs:
                    raw = candidate_output.text.strip()
                    raw = self._strip_thinking(raw)
                    extracted = self._extract_final_answer(raw, 'algebra')
                    candidates.append(extracted)

                # Majority vote among the 3 solutions
                answer = self._majority_vote(candidates)

                if len(answer) > 5000:
                    answer = answer[:5000].rsplit('. ', 1)[0] + '.'

                results[idx] = {
                    "questionID": questions[idx]["questionID"],
                    "answer": answer
                }

        # Batch C: General questions (History, Geography, Finance, etc.)
        if general_prompts:
            print(f"ğŸ“– Processing {len(general_prompts)} general questions...")

            general_outputs = self.llm.generate(general_prompts, self.params_general, use_tqdm=False)

            for idx, output in zip(general_indices, general_outputs):
                raw_answer = output.outputs[0].text.strip()

                # Remove "Answer:" prefix if present
                if raw_answer.lower().startswith('answer:'):
                    raw_answer = raw_answer[7:].strip()

                # AGGRESSIVE cleanup for reasoning
                # If output starts with "Okay" and is long (>200 chars), it's all reasoning
                if raw_answer.lower().startswith('okay') and len(raw_answer) > 200:
                    # This is likely full reasoning - extract nothing, mark as failed
                    answer = "Unable to generate answer"
                else:
                    # Split into sentences
                    sentences = re.split(r'(?<=[.!?])\s+', raw_answer)

                    # If first sentence starts with "Okay" or similar, skip it
                    bad_starts = ['okay', 'well', 'alright', 'let me', 'the user', 'i need', 'i will']
                    if sentences and len(sentences) > 1:
                        first_sent_lower = sentences[0].lower()
                        if any(first_sent_lower.startswith(bad) for bad in bad_starts):
                            # Remove first sentence
                            raw_answer = ' '.join(sentences[1:])

                    answer = self._strip_thinking(raw_answer.strip())

                    # Clean up any remaining meta phrases at start
                    if answer.lower().startswith('from what i'):
                        # Extract just the factual part
                        parts = answer.split(',', 1)
                        if len(parts) > 1:
                            answer = parts[1].strip()

                    # If answer is empty, fallback
                    if not answer or len(answer) < 5:
                        answer = "Unable to generate answer"

                if len(answer) > 5000:
                    answer = answer[:5000].rsplit('. ', 1)[0] + '.'

                results[idx] = {
                    "questionID": questions[idx]["questionID"],
                    "answer": answer
                }

        print(f"âœ… Completed {len(results)} questions\n")
        return results

    def _load_chinese_knowledge_base(self) -> str:
        """Load Chinese knowledge base for cache-augmented generation"""
        try:
            kb_path = Path(CHINESE_KB_PATH)
            if kb_path.exists():
                with open(kb_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"ğŸ“š Loaded Chinese knowledge base ({len(content)} chars)")
                return content
            else:
                print("âš ï¸  Chinese knowledge base not found, using default prompts")
                return ""
        except Exception as e:
            print(f"âš ï¸  Error loading Chinese KB: {e}")
            return ""

    def _load_algebra_knowledge_base(self) -> str:
        """Load Algebra knowledge base for formula/theorem reference"""
        try:
            kb_path = Path(ALGEBRA_KB_PATH)
            if kb_path.exists():
                with open(kb_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"ğŸ“ Loaded Algebra knowledge base ({len(content)} chars)")
                return content
            else:
                print("âš ï¸  Algebra knowledge base not found, using default prompts")
                return ""
        except Exception as e:
            print(f"âš ï¸  Error loading Algebra KB: {e}")
            return ""

    def _warmup_cache(self):
        """
        Warmup the KV cache by processing the Knowledge Bases once.
        This ensures subsequent requests using these KBs hit the cache.
        """
        print("ğŸ”¥ Warming up cache with Knowledge Bases...")
        
        warmup_prompts = []
        
        # Chinese KB Warmup
        if self.chinese_kb:
            prompt = f"""ä½ æ˜¯ä¸­å›½æ–‡åŒ–å’Œè¯­è¨€ä¸“å®¶ã€‚ä½¿ç”¨ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

å‚è€ƒèµ„æ–™ï¼š
{self.chinese_kb}

åŸºäºä»¥ä¸Šå‚è€ƒèµ„æ–™ï¼Œç›´æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜: é¢„çƒ­
ç­”æ¡ˆ:"""
            warmup_prompts.append(prompt)
            
        # Algebra KB Warmup
        if self.algebra_kb:
            prompt = f"""You are a math expert. Use the formulas and theorems below to solve the problem.

REFERENCE FORMULAS:
{self.algebra_kb}

Using the above formulas, solve step-by-step and provide the final answer.

Problem: Warmup
Solution:"""
            warmup_prompts.append(prompt)
            
        if warmup_prompts:
            # Run a dummy generation to populate cache
            # We use max_tokens=1 just to process the prompt
            self.llm.generate(warmup_prompts, SamplingParams(max_tokens=1), use_tqdm=False)
            print("âœ… Cache warmed up!")
        else:
            print("âš ï¸ No KBs to warm up.")


def loadPipeline():
    """
    Entry point for evaluation system

    This function is called ONCE before timing starts.
    All quantization happens here (untimed).
    """
    return InferencePipeline()
