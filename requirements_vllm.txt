# Tech Arena 2025 - Optimized vLLM Pipeline Requirements
# Tested on Python 3.10+ with CUDA 11.8+

# Core dependencies
torch==2.5.1
transformers==4.49.0

# vLLM inference engine (critical for performance)
vllm==0.6.3.post1

# FP8 quantization support (requires CUDA 11.8+)
# Built into vLLM 0.6.3+, no additional packages needed

# Utilities
accelerate==1.2.1
sentencepiece==0.2.0
protobuf==5.29.1

# Data handling
numpy>=1.24.0,<2.0.0

# Optional: Monitoring and profiling
psutil>=5.9.0

# Notes:
# - Install PyTorch with CUDA support first:
#   pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu118
# - vLLM requires CUDA 11.8 or 12.1
# - FP8 quantization works best on Ada/Hopper GPUs but supports T4
# - For CPU-only testing, vLLM still requires CUDA libraries (use transformers fallback)
